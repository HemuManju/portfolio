[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "",
    "section": "",
    "text": "Manjunatha, Hemanth, Sri Sadhan Jujjavarapu, and Ehsan T Esfahani. Transfer Learning of Motor Difficulty Classification in Physical Human-Robot Interaction Using Electromyography. Journal of Computing and Information Science in Engineering, 1–32.\nManjunatha, Hemanth, Shrey Pareek, Sri Sadhan Jujjavarapu, Mostafa Ghobadi, Thenkurussi Kesavadas, and Ehsan T Esfahani. 2021. Upper Limb Home-Based Robotic Rehabilitation During COVID-19 Outbreak. Frontiers in Robotics and AI 8.\nManjunatha, Hemanth, Shrey Pareek, Amirhossein H Memar, Thenkurussi Kesavadas, and Ehsan T Esfahani. 2020. Effect of Haptic Assistance Strategy on Mental Engagement in Fine Motor Tasks. Journal of Medical Robotics Research 5 (01n02): 2041004.\nPareek, Shrey, Hemanth Manjunatha, Ehsan T Esfahani, and Thenkurussi Kesavadas. 2019. Myotrack: Realtime Estimation of Subject Participation in Robotic Rehabilitation Using SEMG and IMU. IEEE Access 7: 76030–41.\nThammineni, Chaitanya, Hemanth Manjunatha, and Ehsan T Esfahani. 2021. Selective Eye-Gaze Augmentation to Enhance Imitation Learning in Atari Games. Neural Computing and Applications, 1–10.\nZhang, Binbin, Jida Huang, Rahul Rai, and Hemanth Manjunatha. 2018. A Sequential Sampling Algorithm for Multistage Static Coverage Problems. Journal of Computing and Information Science in Engineering 18 (2)."
  },
  {
    "objectID": "publications/index.html#journal-papers",
    "href": "publications/index.html#journal-papers",
    "title": "",
    "section": "",
    "text": "Manjunatha, Hemanth, Sri Sadhan Jujjavarapu, and Ehsan T Esfahani. Transfer Learning of Motor Difficulty Classification in Physical Human-Robot Interaction Using Electromyography. Journal of Computing and Information Science in Engineering, 1–32.\nManjunatha, Hemanth, Shrey Pareek, Sri Sadhan Jujjavarapu, Mostafa Ghobadi, Thenkurussi Kesavadas, and Ehsan T Esfahani. 2021. Upper Limb Home-Based Robotic Rehabilitation During COVID-19 Outbreak. Frontiers in Robotics and AI 8.\nManjunatha, Hemanth, Shrey Pareek, Amirhossein H Memar, Thenkurussi Kesavadas, and Ehsan T Esfahani. 2020. Effect of Haptic Assistance Strategy on Mental Engagement in Fine Motor Tasks. Journal of Medical Robotics Research 5 (01n02): 2041004.\nPareek, Shrey, Hemanth Manjunatha, Ehsan T Esfahani, and Thenkurussi Kesavadas. 2019. Myotrack: Realtime Estimation of Subject Participation in Robotic Rehabilitation Using SEMG and IMU. IEEE Access 7: 76030–41.\nThammineni, Chaitanya, Hemanth Manjunatha, and Ehsan T Esfahani. 2021. Selective Eye-Gaze Augmentation to Enhance Imitation Learning in Atari Games. Neural Computing and Applications, 1–10.\nZhang, Binbin, Jida Huang, Rahul Rai, and Hemanth Manjunatha. 2018. A Sequential Sampling Algorithm for Multistage Static Coverage Problems. Journal of Computing and Information Science in Engineering 18 (2)."
  },
  {
    "objectID": "publications/index.html#conference-paper",
    "href": "publications/index.html#conference-paper",
    "title": "",
    "section": "Conference Paper",
    "text": "Conference Paper\n\nAmir, Behjat, Hemanth Manjunatha, Apurv J. Kumar, Leighton Collins, Payam Ghassemi, Souma Chowdhury, Karthik Dantu, David Doermann, and Ehsan T. Esfahani. 2021. Learning Swarm Tactics Overs Complex Uncertain Environments. In 2021 International Symposium on Multi-Robot and Multi-Agent Systems (MRS).\nDistefano, Joseph P., Hemanth Manjunatha, Souma Chowdhury, Karthik Dantu, David Doermann, and Ehsan T. Esfahani. 2021. Using Physiological Information to Classify Task Difficulty in Human-Swarm Interaction. IEEE/SMC International Conference on System of Systems Engineering.\nManjunatha, Hemanth, Joseph P. Distefano, Payam Ghassemi, Souma Chowdhury, Karthik Dantu, David Doermann, and Ehsan T. Esfahani. 2020. Using Physiological Measurements to Analyze the Tactical Decisions in Human Swarm Teams. IEEE/SMC International Conference on System of Systems Engineering.\nManjunatha, Hemanth, Joseph P Distefano, Apurv Jani, Payam Ghassemi, Souma Chowdhury, Karthik Dantu, David Doermann, and Ehsan T Esfahani. Using Physiological Measurements to Analyze the Tactical Decisions in Human Swarm Teams. In 2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 256–61. IEEE.\nManjunatha, Hemanth, Joseph Distefano, Apurv Jani, Amir Behjat, Payam Ghassemi, David Doermann, Souma Chowdhury, Karthik Dantu, and Ehsan T. Esfahani. 2021. SHaSTA: A Simulator for Human and Swarm Team Applications. In.\nManjunatha, Hemanth, and Ehsan T Esfahani.Extracting Interpretable EEG Features from a Deep Learning Model to Assess the Quality of Human-Robot Co-Manipulation. In 2021 10th International IEEE/EMBS Conference on Neural Engineering (NER), 339–42. IEEE.\nManjunatha, Hemanth, Jida Huang, Binbin Zhang, and Rahul Rai. 2016. A Sequential Sampling Algorithm for Multi-Stage Static Coverage Problems. In International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, 50114:V02BT03A029. American Society of Mechanical Engineers.\nManjunatha, Hemanth, Sri Sadhan Jujjavarapu, and Ehsan T Esfahani. 2020a. Classification of Motor Control Difficulty Using EMG in Physical Human-Robot Interaction. In 2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 2708–13. IEEE."
  },
  {
    "objectID": "publications/index.html#book-chapter",
    "href": "publications/index.html#book-chapter",
    "title": "",
    "section": "Book Chapter",
    "text": "Book Chapter\n\nManjunatha, Hemanth, and Ehsan T Esfahani. 2020. Application of Reinforcement and Deep Learning Techniques in Brain–Machine Interfaces. Advances in Motor Neuroprostheses, 1–14."
  },
  {
    "objectID": "publications/index.html#under-review",
    "href": "publications/index.html#under-review",
    "title": "",
    "section": "Under Review",
    "text": "Under Review\n\nPak, Andrey, Hemanth Manjunatha, Dimitar Filev, and Panagiotis Tsiotras CARNet: A Dynamic Autoencoder for Learning Latent Dynamics in Autonomous Driving Tasks. arXiv Preprint arXiv:2205.08712.\nManjunatha, Hemanth, Amir Memar, and Ehsan Esfahani. 2018. Classification of Task Type and Reaction Time of Operator in Simulated Multiple Robot Tele-Exploration. Frontiers in Human Neuroscience 12."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nClassification of Reaction Time in Simulated Multiple Robot Tele-Exploration\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2019\n\n\nHemanth Manjunatha\n\n\n\n\n\n\n\n\n\n\n\n\nExtracting Interpretable EEG Features from a Deep Learning Model to Assess the Quality of Human-Robot Co-manipulation\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2019\n\n\nHemanth Manjunatha\n\n\n\n\n\n\n\n\n\n\n\n\nSHASTA A Simulator for Human and Swarm Team Applications\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2019\n\n\nHemanth Manjunatha\n\n\n\n\n\n\n\n\n\n\n\n\nSelective Eye-gaze Augmentation To Enhance Imitation Learning In Atari Games\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2019\n\n\nHemanth Manjunatha\n\n\n\n\n\n\n\n\n\n\n\n\nTask Difficulty Prediction in Physical Human Interaction using EEG and Deep Learning\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2019\n\n\nHemanth Manjunatha\n\n\n\n\n\n\n\n\n\n\n\n\nTransfer Learning of Motor Difficulty Classification in Physical Human-Robot Interaction Using Electromyography\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2019\n\n\nHemanth Manjunatha\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Physiological Measurements to Analyze the Tactical Decisions in Human Swarm Teams\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2019\n\n\nHemanth Manjunatha\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/tele-operation/index.html",
    "href": "research/tele-operation/index.html",
    "title": "Classification of Reaction Time in Simulated Multiple Robot Tele-Exploration",
    "section": "",
    "text": "In the teleoperated scenario, as the task difficulty increases, the performance of the operator decreases which leads to a decrease in the overall system efficiency. Thus, it is important to predict the change in task difficulty in order to increase system efficiency. However, the task difficulty cannot be predicted as task information is unknown in real-time. Alternatively, the task difficulty can be estimated studying the distribution of reaction time. In this study, the physiological features of the operator are used to classify the reaction time as fast, normal and slow corresponding different levels of task difficulty. The physiological features are extracted from the eye (though eye tracking) and brain (through Electroencephalogram) from the operator performing teleoperation using two drones. Among the calculated features glance ratio and mental workload resulted in maximum classification accuracy when task type information is included."
  },
  {
    "objectID": "research/tele-operation/index.html#abstract",
    "href": "research/tele-operation/index.html#abstract",
    "title": "Classification of Reaction Time in Simulated Multiple Robot Tele-Exploration",
    "section": "",
    "text": "In the teleoperated scenario, as the task difficulty increases, the performance of the operator decreases which leads to a decrease in the overall system efficiency. Thus, it is important to predict the change in task difficulty in order to increase system efficiency. However, the task difficulty cannot be predicted as task information is unknown in real-time. Alternatively, the task difficulty can be estimated studying the distribution of reaction time. In this study, the physiological features of the operator are used to classify the reaction time as fast, normal and slow corresponding different levels of task difficulty. The physiological features are extracted from the eye (though eye tracking) and brain (through Electroencephalogram) from the operator performing teleoperation using two drones. Among the calculated features glance ratio and mental workload resulted in maximum classification accuracy when task type information is included."
  },
  {
    "objectID": "research/tele-operation/index.html#architecture",
    "href": "research/tele-operation/index.html#architecture",
    "title": "Classification of Reaction Time in Simulated Multiple Robot Tele-Exploration",
    "section": "Architecture",
    "text": "Architecture\nThe study set out to answer two questions:\n\nWhich physiological modalities, eye, brain activity, or their combination can provide a more accurate classification of the reaction time?\nDo the task type information and individual differences in the task performance influence the classification of reaction time?\n\nThe study concluded that both eye features and cognitive features are needed to classify the reaction time effectively. The task type information had a significant influence on the classification accuracy, while the same could not be said with individual difference information. The accuracy of classification was increased when task type information was included as features, indicating that the reaction time depends on the task information.\nThese results are in agreement with the Linear Approach to the Threshold with Ergodic Rate (LATER) model, which suggests that reaction time depends on evidence collected and the task difficulty. The classifier trained on the physiological features using reaction time as the label can be used to classify task difficulty well above chance. Such systems can be utilized in Automation Invocation models that utilize appropriate control inputs and control strategies for optimal role allocation in collaborative tasks such as teleoperation.\nThe present work showed that physiological measurements could indeed be used to model the task difficulty through auxiliary measurements such as reaction time"
  },
  {
    "objectID": "research/explainability/index.html",
    "href": "research/explainability/index.html",
    "title": "Extracting Interpretable EEG Features from a Deep Learning Model to Assess the Quality of Human-Robot Co-manipulation",
    "section": "",
    "text": "There is an increasing interest in adapting the deep learning models into neuroimaging techniques such as electroencephalogram (EEG). However, one of the fundamental problems in deep learning models is the interpretability of the learned representations. Even though many interpretability models exist for computer vision applications, adapting those methods for deep learning using EEG is still a challenge. In this regard, we propose a novel computational approach to increase the interpretability of results from deep learning algorithm using two popular saliency detection algorithms: integrated gradients and ablation attribution method. The method provides the importance of values across different EEG frequency bands (Theta, Alpha, Beta, Gamma) and across different electrode locations. We can use these importance values to recognize which electrode and frequency bands are relevant for a particular classification problem. We demonstrate the proposed method’s efficacy in a physical human-robot co-manipulation experiment where a convolution neural network (CNN) model is trained to classify the user’s mental workload using raw EEG recordings. The experiment is predominantly visuospatial and motor control-oriented. The proposed method found the Gamma and Beta frequency band across parietal and occipital regions to be important, which are indeed associated with visuospatial processing and sensory integration."
  },
  {
    "objectID": "research/explainability/index.html#abstract",
    "href": "research/explainability/index.html#abstract",
    "title": "Extracting Interpretable EEG Features from a Deep Learning Model to Assess the Quality of Human-Robot Co-manipulation",
    "section": "",
    "text": "There is an increasing interest in adapting the deep learning models into neuroimaging techniques such as electroencephalogram (EEG). However, one of the fundamental problems in deep learning models is the interpretability of the learned representations. Even though many interpretability models exist for computer vision applications, adapting those methods for deep learning using EEG is still a challenge. In this regard, we propose a novel computational approach to increase the interpretability of results from deep learning algorithm using two popular saliency detection algorithms: integrated gradients and ablation attribution method. The method provides the importance of values across different EEG frequency bands (Theta, Alpha, Beta, Gamma) and across different electrode locations. We can use these importance values to recognize which electrode and frequency bands are relevant for a particular classification problem. We demonstrate the proposed method’s efficacy in a physical human-robot co-manipulation experiment where a convolution neural network (CNN) model is trained to classify the user’s mental workload using raw EEG recordings. The experiment is predominantly visuospatial and motor control-oriented. The proposed method found the Gamma and Beta frequency band across parietal and occipital regions to be important, which are indeed associated with visuospatial processing and sensory integration."
  },
  {
    "objectID": "research/explainability/index.html#results",
    "href": "research/explainability/index.html#results",
    "title": "Extracting Interpretable EEG Features from a Deep Learning Model to Assess the Quality of Human-Robot Co-manipulation",
    "section": "Results",
    "text": "Results\nThe study set out to answer two questions:\n\nWhich physiological modalities, eye, brain activity, or their combination can provide a more accurate classification of the reaction time?\nDo the task type information and individual differences in the task performance influence the classification of reaction time?\n\nThe study concluded that both eye features and cognitive features are needed to classify the reaction time effectively. The task type information had a significant influence on the classification accuracy, while the same could not be said with individual difference information. The accuracy of classification was increased when task type information was included as features, indicating that the reaction time depends on the task information.\nThese results are in agreement with the Linear Approach to the Threshold with Ergodic Rate (LATER) model, which suggests that reaction time depends on evidence collected and the task difficulty. The classifier trained on the physiological features using reaction time as the label can be used to classify task difficulty well above chance. Such systems can be utilized in Automation Invocation models that utilize appropriate control inputs and control strategies for optimal role allocation in collaborative tasks such as teleoperation.\nThe present work showed that physiological measurements could indeed be used to model the task difficulty through auxiliary measurements such as reaction time"
  },
  {
    "objectID": "research/task-difficulty/index.html",
    "href": "research/task-difficulty/index.html",
    "title": "Task Difficulty Prediction in Physical Human Interaction using EEG and Deep Learning",
    "section": "",
    "text": "An experiment was conducted in which the subjects have to guide an robot under admittance control through a predefined trajectory. The damping in the admittance control was changed from low to high during different trials. Due to low damping, the controller might become unstable during the force input from the human subject. To quantify the instability, a frequency domain measure has been extracted from the force data. During the whole experiment scalp EEG was recorded and used to predict whether the robot becomes unstable during the co-manipulation. This was achieved by constructing a regression with EEG features as input and instability index as output. For the prediction model, we are leveraging the deep learning technique namely convolution neural networks."
  },
  {
    "objectID": "research/task-difficulty/index.html#abstract",
    "href": "research/task-difficulty/index.html#abstract",
    "title": "Task Difficulty Prediction in Physical Human Interaction using EEG and Deep Learning",
    "section": "",
    "text": "An experiment was conducted in which the subjects have to guide an robot under admittance control through a predefined trajectory. The damping in the admittance control was changed from low to high during different trials. Due to low damping, the controller might become unstable during the force input from the human subject. To quantify the instability, a frequency domain measure has been extracted from the force data. During the whole experiment scalp EEG was recorded and used to predict whether the robot becomes unstable during the co-manipulation. This was achieved by constructing a regression with EEG features as input and instability index as output. For the prediction model, we are leveraging the deep learning technique namely convolution neural networks."
  },
  {
    "objectID": "research/shasta/index.html#abstract",
    "href": "research/shasta/index.html#abstract",
    "title": "SHASTA A Simulator for Human and Swarm Team Applications",
    "section": "Abstract",
    "text": "Abstract\nHuman-Swarm Teams uniquely combine the ability of swarms and cognition of humans to achieve complex behaviors. There is a need for a human-in-the-loop simulation framework that can capture both the human’s action and cognition while interacting with the swarm of robots as well as the actions of the swarm itself to study the unique characteristics of a human-swarm team. In this paper, we present , an open-source simulation platform to study human-swarm applications. The platform provides a unified interface for humans and learning algorithms to command/learn swarm robot behaviors. The higher level swarm behavior is achieved by including single robot and swarm primitives such as formation control and path planning. SHASTA provides a unique ability to collect physiological information from humans and time synchronize it with swarm simulation. Using search-and-rescue as an example, we demonstrate the scalability and utility ofSHASTA in simulating human-swarm applications. We also demonstrate its ability to integrate physiological measurements as part of the simulation through a detailed human-study."
  },
  {
    "objectID": "research/shasta/index.html#results",
    "href": "research/shasta/index.html#results",
    "title": "SHASTA A Simulator for Human and Swarm Team Applications",
    "section": "Results",
    "text": "Results\n\n\n\nThe present work showed that physiological measurements could indeed be used to model the task difficulty through auxiliary measurements such as reaction time"
  },
  {
    "objectID": "research/transfer-learning/index.html#abstract",
    "href": "research/transfer-learning/index.html#abstract",
    "title": "Transfer Learning of Motor Difficulty Classification in Physical Human-Robot Interaction Using Electromyography",
    "section": "Abstract",
    "text": "Abstract\nEfficient human-robot collaboration during physical interaction requires estimating the human state for optimal role allocation and load sharing. Machine learning (ML) methods are gaining popularity for estimating the interaction parameters from physiological signals. However, due to individual differences, the ML models might not generalize well to new subjects. In this study, we present a convolution neural network (CNN) model to predict motor control difficulty using surface electromyography (sEMG) from human upper-limb during physical human-robot interaction (pHRI) task and present a transfer learning approach to transfer a learned model to new subjects.\nTwenty six individuals participated in a pHRI experiment where a subject guides the robot’s end-effector with different levels of motor control difficulty. The motor control difficulty is varied by changing the damping parameter of the robot from low to high and constraining the motion to gross and fine movements. A CNN network with raw sEMG as input is used to classify the motor control difficulty.\nThe CNN’s transfer learning approach is compared against Riemann geometry-based Procrustes analysis (RPA). With very few labeled samples from new subjects, we demonstrate that the CNN-based transfer learning approach (avg. 69.77%) outperforms the RPA transfer learning (avg. 59.20%).\nMoreover, we observe that the subject’s skill level in the pre-trained model has no significant effect on the transfer learning performance of the new users."
  },
  {
    "objectID": "research/transfer-learning/index.html#results",
    "href": "research/transfer-learning/index.html#results",
    "title": "Transfer Learning of Motor Difficulty Classification in Physical Human-Robot Interaction Using Electromyography",
    "section": "Results",
    "text": "Results\n\n\n\nThe study set out to answer two questions:\n\nWhich physiological modalities, eye, brain activity, or their combination can provide a more accurate classification of the reaction time?\nDo the task type information and individual differences in the task performance influence the classification of reaction time?\n\nThe study concluded that both eye features and cognitive features are needed to classify the reaction time effectively. The task type information had a significant influence on the classification accuracy, while the same could not be said with individual difference information. The accuracy of classification was increased when task type information was included as features, indicating that the reaction time depends on the task information.\nThese results are in agreement with the Linear Approach to the Threshold with Ergodic Rate (LATER) model, which suggests that reaction time depends on evidence collected and the task difficulty. The classifier trained on the physiological features using reaction time as the label can be used to classify task difficulty well above chance. Such systems can be utilized in Automation Invocation models that utilize appropriate control inputs and control strategies for optimal role allocation in collaborative tasks such as teleoperation.\n\nThe present work showed that physiological measurements could indeed be used to model the task difficulty through auxiliary measurements such as reaction time"
  },
  {
    "objectID": "research/human-swarm-decision/index.html",
    "href": "research/human-swarm-decision/index.html",
    "title": "Using Physiological Measurements to Analyze the Tactical Decisions in Human Swarm Teams",
    "section": "",
    "text": "Abstract\nHuman-Swarm interaction has attracted a lot of attention for their applications in areas such as exploration, rescue, surveillance, and interplanetary exploration. When humans assume a supervisory or tactician role in managing the robot swarm, the humans’ (physiological) state significantly affects the mission performance. In this work, we explore the physiological correlates with the user’s tactical decisions in a simulated search and rescue mission. The mission consists of supervising three groups of unmanned aerial vehicles and three groups of unmanned ground vehicles to search for a target building. The mission complexity is increased by introducing static adversarial teams. Due to the adversarial team’s presence, the user should employ different tactics to search for a target. While the user interacts with the swarm, brain activity in forms of electroencephalogram (EEG) and eye movements are recorded. 20 participants, with prior experience in playing real-time strategy games, took part in the study. A linear mixed effect model is used to study the correlated physiological features and tactical decisions. Six features are extracted from the physiological data: engagement level, mental workload, Fz-Pz coherence, Fz-O1 coherence, pupil size, and the number of gaze fixations. The results show that mental engagement and Fz-O1 coherence are the important factors in predicting the tactical decisions. Specifically, Fz-O1 coherence in Beta (22.5-30 Hz) and Gamma (38-42 Hz) band is found to be significant."
  },
  {
    "objectID": "research/eye-gaze-augmentation/index.html#abstract",
    "href": "research/eye-gaze-augmentation/index.html#abstract",
    "title": "Selective Eye-gaze Augmentation To Enhance Imitation Learning In Atari Games",
    "section": "Abstract",
    "text": "Abstract\nThis paper presents the selective use of eye-gaze information in learning human actions in Atari games. Extensive evidence suggests that our eye movements convey a wealth of information about the direction of our attention and mental states and encode the information necessary to complete a task. Based on this evidence, we hypothesize that selective use of eye-gaze, as a clue for attention direction, will enhance the learning from demonstration. For this purpose, we propose a selective eye-gaze augmentation (SEA) network that learns when to use the eye-gaze information. The proposed network architecture consists of three sub-networks: gaze prediction, gating, and action prediction network. Using the prior 4 game frames, a gaze map is predicted by the gaze prediction network, which is used for augmenting the input frame. The gating network will determine whether the predicted gaze map should be used in learning and is fed to the final network to predict the action at the current frame. To validate this approach, we use publicly available Atari Human Eye-Tracking And Demonstration (Atari-HEAD) dataset consists of 20 Atari games with 28 million human demonstrations and 328 million eye-gazes (over game frames) collected from four subjects. We demonstrate the efficacy of selective eye-gaze augmentation compared to the state of the art Attention Guided Imitation Learning (AGIL), Behavior Cloning (BC).\nThe results indicate that the selective augmentation approach (the SEA network) performs significantly better than the AGIL and BC. Moreover, to demonstrate the significance of selective use of gaze through the gating network, we compare our approach with the random selection of the gaze. Even in this case, the SEA network performs significantly better, validating the advantage of selectively using the gaze in demonstration learning."
  },
  {
    "objectID": "research/eye-gaze-augmentation/index.html#results",
    "href": "research/eye-gaze-augmentation/index.html#results",
    "title": "Selective Eye-gaze Augmentation To Enhance Imitation Learning In Atari Games",
    "section": "Results",
    "text": "Results\nThe present work showed that physiological measurements could indeed be used to model the task difficulty through auxiliary measurements such as reaction time"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": Hemanth Manjunatha,
    "section": "",
    "text": "I am a postdoctoral researcher at Georgia Institute of Techonolgy exploring Reinforcement Learning, Human Robot/Swarm Interaction, and Brain Computer Interfaces. My long-term goal is to contribute towards developing science that enables humans and machines to be a synergistic team."
  },
  {
    "objectID": "index.html#hello",
    "href": "index.html#hello",
    "title": Hemanth Manjunatha,
    "section": "",
    "text": "I am a postdoctoral researcher at Georgia Institute of Techonolgy exploring Reinforcement Learning, Human Robot/Swarm Interaction, and Brain Computer Interfaces. My long-term goal is to contribute towards developing science that enables humans and machines to be a synergistic team."
  },
  {
    "objectID": "index.html#research-goals",
    "href": "index.html#research-goals",
    "title": Hemanth Manjunatha,
    "section": "Research Goals",
    "text": "Research Goals\n\nDeveloping theories for what levels and types of human modeling are needed for effective teaming with machines, specifically by considering neurophysiological modalities\nDeveloping learning algorithms that enable machines to learn and adapt their behavior directly from human intentions, demonstration, and instructions\nDeveloping learning algorithms that learn from human agents and provide an explanation at different levels of abstraction in complex collaborative tasks."
  }
]